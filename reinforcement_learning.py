# -*- coding: utf-8 -*-
"""Reinforcement_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ngLa4dBs5ueAt2kHE2bdY-OwKp8TVVKd
"""

import gym
from gym import wrappers
import pandas
import matplotlib.pyplot as plt


import warnings
warnings.simplefilter('ignore')

def render_env(env):
  plt.imshow(env.render(mode='rgb_array'))
  plt.axis('off')
  plt.show()

"""**FIRST PART OF GAME USING FROZENLAKE-V0 ENVIRONMENT**

Objective of game : Small world with 16 tiles:

          SFFF
          FHFH
          FFFH
          HFFG

Game start with S tile and to get to the goal 'G' tile , no stepping on 'H' tile.
"""

#BUILDING ENVIRONMENT WITH GYM.MAKE
env = gym.make('FrozenLake-v1') #build a fresh environment.

#START A NEW GAME WITH ENV.RESET()
current_observation = env.reset() #this starts with a new episode and returns the initial observation.

#the current observation is just the current location
print(current_observation)

env.env?

render_env(env)

# the action space for this environment includes 4 discrete actions
print(f"our action space : {env.action_space}")
new_action = env.action_space.sample() #randomly chooses one of action
print(f"our new action: {new_action}")

new_action = env.action_space.sample()

observation, reward, done, info = env.step(new_action)

print(f"observation : {observation}, reward : {reward}, done : {done}, info :{info}")
render_env(env)

#lets put this process in continuous fashion

current_observation = env.reset()

for i in range(5):
  new_action = env.action_space.sample() #create random action
  observation, reward, done, info = env.step(new_action)

  #gathering all the data from action
  print(f"Observation : {observation}, reward: {reward}, done : {done}, info: {info}")
  render_env(env)

#simulatio of whole episode
#rendering every time till it reaches its goal

current_observation = env.reset()
done = False

while not done:
  new_action = env.action_space.sample()
  new_observation, reward, done, info = env.step(new_action)
  print(f"action : {new_action}, observation : {new_observation}, reward : {reward}, done : {done}, info : {info}")

#GATHERING DATA FOR FURTHER TRAINING\PROCESSING
import numpy as np

env = gym.make('FrozenLake-v1')

num_episodes = 40000

life_memory = []
for i in range(num_episodes):

  #starting new episodes and recodring all that memories
  old_observation = env.reset()
  done = False
  tot_reward = 0
  ep_memory = []
  while not done:
    new_action = env.action_space.sample()
    observation, reward, done, info = env.step(new_action)
    tot_reward += reward

    ep_memory.append({
        'observation' : old_observation,
        'action' : new_action,
        'reward' : reward,
        'episode' : i,
    })
    old_observation = observation


  #incorporate total_reward
  num_steps = len(ep_memory)
  for i,ep_mem in enumerate(ep_memory):
    ep_mem['tot_reward'] = tot_reward
    ep_mem['decay_reward'] = i*tot_reward/num_steps

  life_memory.extend(ep_memory)

memory_df = pandas.DataFrame(life_memory)

memory_df

memory_df.describe()

np.mean(memory_df.groupby('episode').reward.sum())

#so success rate with randomisation process is just 1.34%
#here is why RL models are used

#Using supervised models to leaverage from data gathered and predict outcomes

from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.svm import SVR

model = ExtraTreesRegressor(n_estimators = 50)
#model = SVR()
y = 0.5*memory_df.reward + 0.1*memory_df.decay_reward + memory_df.tot_reward
x = memory_df[['observation' , 'action']]
model.fit(x,y)

model1 = RandomForestRegressor()
y = 1*memory_df.reward + memory_df.tot_reward + 0.1*memory_df.decay_reward
x = memory_df[['observation' , 'action']]
model1.fit(x,y)

num_episodes = 500

life_memory = []
for i in range(num_episodes):

  #starting new episodes and record all its memory
  old_observation = env.reset()
  done = False
  tot_reward = 0
  ep_memory = []
  while not done:

    pred_in = [[old_observation, i] for i in range(4)]
    new_action = np.argmax(model1.predict(pred_in))
    observation, reward, done, info = env.step(new_action)
    tot_reward += reward

    ep_memory.append({
        'observation' : old_observation,
        'action' : new_action,
        'reward' : reward,
        'episode' : i,
    })
    old_observation = observation

    #incorporate total reward
    for ep_mem in ep_memory:
      ep_mem['tot_reward'] = tot_reward

    life_memory.extend(ep_memory)

memory_df2 = pandas.DataFrame(life_memory)

np.mean(memory_df2.groupby('episode').reward.sum())

#we got a very huge boost in performance i.e from 1.3% to straight 66.4%
#lets extend this for other models

num_episodes = 500

life_memory = []
for i in range(num_episodes):

  #starting new episodes and record all its memory
  old_observation = env.reset()
  done = False
  tot_reward = 0
  ep_memory = []
  while not done:

    pred_in = [[old_observation, i] for i in range(4)]
    new_action = np.argmax(model.predict(pred_in))
    observation, reward, done, info = env.step(new_action)
    tot_reward += reward

    ep_memory.append({
        'observation' : old_observation,
        'action' : new_action,
        'reward' : reward,
        'episode' : i,
    })
    old_observation = observation

    #incorporate total reward
    for ep_mem in ep_memory:
      ep_mem['tot_reward'] = tot_reward

    life_memory.extend(ep_memory)

memory_df3 = pandas.DataFrame(life_memory)

np.mean(memory_df3.groupby('episode').reward.sum())

#here too performace has exceeded but similar as for ExtraTreesRegressor
#but with randomisation and intelligence combined like in
#ExtraTreesRegressor as compared to Regular treeregressor.

env = gym.make('CartPole-v0')
env.env?

num_episodes = 1000

life_memory = []
for i in range(num_episodes):

  #recording all memories
  old_observation = env.reset()
  done = False
  tot_reward = 0
  ep_memory = []
  while not done:
    new_action = env.action_space.sample()
    observation, reward, done, info = env.step(new_action)
    tot_reward +=reward

    ep_memory.append({
        'obs0' : old_observation[0],
        'obs1' : old_observation[1],
        'obs2' : old_observation[2],
        'obs3' : old_observation[3],
        'action' : new_action,
        'reward' : reward,
        'episode' : i,
    })

    old_observation = observation

    #incorporate total model
    for ep_mem in ep_memory:
      ep_mem['tot_reward'] = tot_reward

    life_memory.extend(ep_memory)

memory_df = pandas.DataFrame(life_memory)

np.mean(memory_df.groupby('episode').reward.sum())

memory_df

memory_df.describe()

from sklearn.ensemble import RandomForestRegressor , AdaBoostRegressor , ExtraTreesRegressor

model = ExtraTreesRegressor(n_estimators = 50)

memory_df['comb_reward'] = 0.5*memory_df.reward + memory_df.tot_reward
model.fit(memory_df[['obs0' , 'obs1' , 'obs2' , 'obs3' , 'action']], memory_df.comb_reward)

num_episodes = 500

life_memory = []
for i in range(num_episodes):

  #starting new episodes and record all its memory
  old_observation = env.reset()
  done = False
  tot_reward = 0
  ep_memory = []
  while not done:

    pred_in = [list(old_observation) + [i] for i in range(2)]
    new_action = np.argmax(model.predict(pred_in))
    observation, reward, done, info = env.step(new_action)
    tot_reward += reward

    ep_memory.append({
        'obs0' : old_observation[0],
        'obs1' : old_observation[1],
        'obs2' : old_observation[2],
        'obs3' : old_observation[3],
        'action' : new_action,
        'reward' : reward,
        'episode' : i,
    })
    old_observation = observation

    #incorporate total reward
    for ep_mem in ep_memory:
      ep_mem['tot_reward'] = tot_reward

    life_memory.extend(ep_memory)

memory_df = pandas.DataFrame(life_memory)

np.mean(memory_df.groupby('episode').reward.sum())

memory_df[memory_df.tot_reward == memory_df.tot_reward.max()]

